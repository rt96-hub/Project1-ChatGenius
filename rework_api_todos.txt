Ensure the main message endpoint is used for both regular messages, replies, and messages with files
move s3 initialization out of routers, let the s3 client be called into files.py and messages.py and search.py if needed, pass client, as well as bucket_name variable
find reactions endpoints on the frontend and get them to match new put and delete endpoints

respond to the sidebar messages, should it search for more related messages after each message?
when the user engages in a multi turn conversation with the ai we need to pass in the entire conversation in to an ai response function
create a version that lets you tag a user and then search for them in the channel
each channel has a setting for the agent personality
tweak the summarize messages function to be more concise, and group the messages in fewer bullets, additionally standarize the formatting of the messages

dark mode?

when a user is away or offline, dms will be replied to by the ai
additionally, user can chat with their own ai in a special channel

attach file id to vector metadata
add type: "message"/"file" to metadata
in embedding_service add function embed_file or something, has cases for file_type: jpg, png, pdf, docx, txt, etc
store file vectors in pinecone, create a metadata field for file_id and type: "file" file_name: "file_name"
    - either text or image, we need to send it to OpenAI, have it generate a summary, then encode that summary

DM comes to user, if they are away, the ai will reply to them
    x determine if user is away via endpoint /users/{user_id}/connection-status
    x add a new bool column to the messages table, called from_ai, default false
    NOPE (how does the system know which endpoint to use?) Do we need to make the frontend recognize if the user is away? (get /users/{user_id}/status, returns away, online, offline, (custom no ai respone status?), depending on the status, the message is routed to the appropriate endpoint?)
    x if the user is away from above, the message is sent to /ai/persona/{user_id} with the request containing the senders_id
    x the channels available to the user are passed in to pinecone, or do we need to encode the channel name along with the username in the message?
    x but the list of all message from the channels are passed directly to the ai? no the channels are used to filter the rag search
    x additionally, grab the last X messages within the last Y days from the dm chat itself, construct the messages list in OpenAI chat completion with those
??? - along with above, do we need to pass in all the context from the prior messages? in that case we need to store it somewhere, but that may be too much context
    x generate an ai response, put that in the endpoint below
    x get the message response from the ai, then send it as a message to the dm, tagged as from_ai: true
    x filter to remove the most recent message from the vectors returned by the search
    - tweak the ai to have the users tone and style (this may be difficult, we have to get a bunch of messages the receiver has sent in the app and use that as context or something? but could be too much context)

need to update dms in the frontend to handle this:
    x in the sidebar, the name of the dm needs to be changed to the name of the other user
    x dm is not deletable, and we need to make sure the privacy setting tab is removed
    x also remove the ability to remove a user from a dm
    x update the chatmessage component to handle the from_ai message with a symbol or something
    x when a message is sent in a dm, the backend uses the main message endpoint with logic for dm and user away to generate an ai response
    x can a user edit their ai response? should we allow that? or allow them to delete it?
    x when ai message is broadcasted, the chatmessage component should be able to handle the from_ai message with a symbol or something, right now only on relaod
    - when the user sends the message, it should disappear from the chat immediately
    - maybe a loading state for the ai message?

additionally a user can chat with their own ai in a special channel:
    x need to add a column to the channels table, called ai_channel, default false, this will let it be filtered out in the sidebar
    x have to create a new default channel for the user, called??? this will be created when the user logs in, if it doesnt already exist
    x need an endpoint that searches for the ai channel, and if it doesnt exist, create it, the frontend will call this when loading the sidebar
    x the channel will be marked as a dm, with only the user and their ai. in the backend it will be name-ai-dm, in the frontend, it will be Users AI
    x when the user sends a message, it will be sent to the same message endpoint as a message, and same with the dm, but the sender_id and receiver_id will be the user_id
    x may need to check that there is only one user_id in the channel, this happens before the check receiver away logic(we check if the channel is a dm, then if it is ai, else go to normal dm check)
    - the prompt for the ai here might be slightly different, maybe more personable or something (we could check if the sender and receiver ids are the same)

ai sidebar enhancements:
- ability to delete conversation from the ai sidebar
    x this will use the DELETE /ai/channels/{channel_id}/conversations/{conversation_id} endpoint
    x i think we need to add this endpoint, might have a crud function already
- ability to continue a conversation from the ai sidebar
    x first we have to be able to select the conversation itself, it gets expanded, and a text box added to the bottom, the main ai sidebar text box needs to be collapsed
    x this will use the POST /ai/channels/{channel_id}/conversations/{conversation_id}/messages endpoint
    x and will need to include the prior messages in the conversation, in addtion to another rag search (add conversation history to the ai_query_response function)



ai persona talk like the user features
    - add a "user_profile_prompt" column to the users table, this will be a string that will be used to generate the ai persona
    - when the user disconnects, we need to send a request to openai to generate instructions for future ai responses to sound like the user
    - something like "Please give a short description of the way this user talks, how long are the words they use, do they use punctuation, are they verbose, long messages, short messages... Please create a short instruction set that would provide an LLM the manner in which they should reply to mimic this user's style of typing."
    - anytime the user persona is called, we need to pass that instruction to the system prompt or something 


RESEARCH:
    - context better before or after the message chain?

summarize a thread
    - a small button next to the show/hide replies button, when clicked will show text bubble with the summary
    - maybe include rag here?


tweak prompts across the app
increase the number of messages that are grabbed from the rag search?? more than 5?


chat with documents?? add a feature to the frontend where a user can select a document?
ADD the correct parameters to anywhere we use the embedding service for create and upsert!!!!

trim down api responses, 
 - do we need to send all messages when the user logs in? /user/me
 - do we need to send all messages when we load the channel sidebar? /channels/me
 - hell even when we get the channel, it calls all messages in /channels/{channel_id}
    and again in messages/{channel_id}/messages
 - probably more examples
 
when a user reacts to a message, the icon shows as a ? mark??

 add an @ reference to a user in a regular channel, if they are away, their AI will respond.
 - probably only has context of the current channel? Privacy issues?
 - same as dm message, but the available channel is only the current channel

https://docs.pinecone.io/guides/data/understanding-metadata#metadata-query-language
$in	Matches vectors with metadata values that are in a specified array. Example: {"genre": {"$in": ["comedy", "documentary"]}}	String, number
use above with channel ids (probably the intersection of channels the sender and receiver are in)