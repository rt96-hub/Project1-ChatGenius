Ensure the main message endpoint is used for both regular messages, replies, and messages with files
move s3 initialization out of routers, let the s3 client be called into files.py and messages.py and search.py if needed, pass client, as well as bucket_name variable
find reactions endpoints on the frontend and get them to match new put and delete endpoints

respond to the sidebar messages, should it search for more related messages after each message?
when the user engages in a multi turn conversation with the ai we need to pass in the entire conversation in to an ai response function
create a version that lets you tag a user and then search for them in the channel
each channel has a setting for the agent personality
tweak the summarize messages function to be more concise, and group the messages in fewer bullets, additionally standarize the formatting of the messages

dark mode?

when a user is away or offline, dms will be replied to by the ai
additionally, user can chat with their own ai in a special channel

summarize thread
attach file id to vector metadata
add type: "message"/"file" to metadata
in embedding_service add function embed_file or something, has cases for file_type: jpg, png, pdf, docx, txt, etc
store file vectors in pinecone, create a metadata field for file_id and type: "file" file_name: "file_name"
    - either text or image, we need to send it to OpenAI, have it generate a summary, then encode that summary

DM comes to user, if they are away, the ai will reply to them
    x determine if user is away via endpoint /users/{user_id}/connection-status
    x add a new bool column to the messages table, called from_ai, default false
    NOPE (how does the system know which endpoint to use?) Do we need to make the frontend recognize if the user is away? (get /users/{user_id}/status, returns away, online, offline, (custom no ai respone status?), depending on the status, the message is routed to the appropriate endpoint?)
    x if the user is away from above, the message is sent to /ai/persona/{user_id} with the request containing the senders_id
    x the channels available to the user are passed in to pinecone, or do we need to encode the channel name along with the username in the message?
    x but the list of all message from the channels are passed directly to the ai? no the channels are used to filter the rag search
    x additionally, grab the last X messages within the last Y days from the dm chat itself, construct the messages list in OpenAI chat completion with those
??? - along with above, do we need to pass in all the context from the prior messages? in that case we need to store it somewhere, but that may be too much context
    x generate an ai response, put that in the endpoint below
    x get the message response from the ai, then send it as a message to the dm, tagged as from_ai: true
    x filter to remove the most recent message from the vectors returned by the search
    - tweak the ai to have the users tone and style (this may be difficult, we have to get a bunch of messages the receiver has sent in the app and use that as context or something? but could be too much context)

need to update dms in the frontend to handle this:
    x in the sidebar, the name of the dm needs to be changed to the name of the other user
    x dm is not deletable, and we need to make sure the privacy setting tab is removed
    x also remove the ability to remove a user from a dm
    x update the chatmessage component to handle the from_ai message with a symbol or something
    x when a message is sent in a dm, the backend uses the main message endpoint with logic for dm and user away to generate an ai response
    x can a user edit their ai response? should we allow that? or allow them to delete it?
    x when ai message is broadcasted, the chatmessage component should be able to handle the from_ai message with a symbol or something, right now only on relaod
    - when the user sends the message, it should disappear from the chat immediately
    - maybe a loading state for the ai message?

additionally a user can chat with their own ai in a special channel:
    - have to create a new default channel for the user, called??? need a script to create for existing users
    - the channel will be marked as ai_channel: true, and have no ability to be deleted, privacy settings, etc (basically the same as a dm)
    - the ai channel will be used to chat with the user's ai, maybe a slight tweak on the /ai/persona/{user_id}, becomes /ai/persona/me? honestly just keep the same message endpoint logic from dms
    - the prompt for the ai here will be slightly different, maybe more personable or something
    - and maybe this message endpoint will have access to the sidebar ai chats

increase the number of messages that are grabbed from the rag search?? more than 5?

ability to delete conversation from the ai sidebar
ability to continue a conversation from the ai sidebar


chat with documents?? add a feature to the frontend where a user can select a document?
ADD the correct parameters to anywhere we use the embedding service for create and upsert!!!!

trim down api responses, 
 - do we need to send all messages when the user logs in? /user/me
 - do we need to send all messages when we load the channel sidebar? /channels/me
 - hell even when we get the channel, it calls all messages in /channels/{channel_id}
    and again in messages/{channel_id}/messages
 - probably more examples
 
when a user reacts to a message, the icon shows as a ? mark??

 add an @ reference to a user in a regular channel, if they are away, their AI will respond.
 - probably only has context of the current channel? Privacy issues?
 - same as dm message, but the available channel is only the current channel

https://docs.pinecone.io/guides/data/understanding-metadata#metadata-query-language
$in	Matches vectors with metadata values that are in a specified array. Example: {"genre": {"$in": ["comedy", "documentary"]}}	String, number
use above with channel ids (probably the intersection of channels the sender and receiver are in)