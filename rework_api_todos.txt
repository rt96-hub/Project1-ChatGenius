Ensure the main message endpoint is used for both regular messages, replies, and messages with files
move s3 initialization out of routers, let the s3 client be called into files.py and messages.py and search.py if needed, pass client, as well as bucket_name variable
find reactions endpoints on the frontend and get them to match new put and delete endpoints

respond to the sidebar messages, should it search for more related messages after each message?
when the user engages in a multi turn conversation with the ai we need to pass in the entire conversation in to an ai response function
create a version that lets you tag a user and then search for them in the channel
each channel has a setting for the agent personality
tweak the summarize messages function to be more concise, and group the messages in fewer bullets, additionally standarize the formatting of the messages

dark mode?

when a user is away or offline, dms will be replied to by the ai
additionally, user can chat with their own ai in a special channel

summarize thread
attach file id to vector metadata
add type: "message"/"file" to metadata
in embedding_service add function embed_file or something, has cases for file_type: jpg, png, pdf, docx, txt, etc
store file vectors in pinecone, create a metadata field for file_id and type: "file" file_name: "file_name"
    - either text or image, we need to send it to OpenAI, have it generate a summary, then encode that summary

DM comes to user, if they are away, the ai will reply to them
    x determine if user is away via endpoint /users/{user_id}/connection-status
    x add a new bool column to the messages table, called from_ai, default false
    fe (how does the system know which endpoint to use?) Do we need to make the frontend recognize if the user is away? (get /users/{user_id}/status, returns away, online, offline, (custom no ai respone status?), depending on the status, the message is routed to the appropriate endpoint?)
    x if the user is away from above, the message is sent to /ai/persona/{user_id} with the request containing the senders_id
    - the channels available to the user are passed in to pinecone, or do we need to encode the channel name along with the username in the message?
    - but the list of all message from the channels are passed directly to the ai? no the channels are used to filter the rag search
    - additionally, grab the last X messages within the last Y days from the dm chat itself, construct the messages list in OpenAI chat completion with those
    - generate an ai response, put that in the endpoint below
    x get the message response from the ai, then send it as a message to the dm, tagged as from_ai: true
    - tweak the ai to have the users tone and style

need to update dms in the frontend to handle this:
    x in the sidebar, the name of the dm needs to be changed to the name of the other user
    x dm is not deletable, and we need to make sure the privacy setting tab is removed
    x also remove the ability to remove a user from a dm
    - update the chatmessage component to handle the from_ai message with a symbol or something
    - when a message is sent in a dm, the frontend first needs to check if the user is away, using the new endpoint
    - if the user is away, their message is sent to a different endpoint, or the main message endpoint needs to route to the ai response function

additionally a user can chat with their own ai in a special channel:
    - have to create a new default channel for the user, called??? need a script to create for existing users
    - the channel will be marked as ai_channel: true, and have no ability to be deleted, privacy settings, etc
    - the ai channel will be used to chat with the user's ai, maybe a slight tweak on the /ai/persona/{user_id}, becomes /ai/persona/me?
    - the prompt for the ai here will be slightly different, maybe more personable or something
    - and maybe this message endpoint will have access to the sidebar ai chats



ability to delete conversation from the ai sidebar
ability to continue a conversation from the ai sidebar


chat with documents?? add a feature to the frontend where a user can select a document?
ADD the correct parameters to anywhere we use the embedding service for create and upsert!!!!

trim down api responses, 
 - do we need to send all messages when the user logs in? /user/me
 - do we need to send all messages when we load the channel sidebar? /channels/me
 - hell even when we get the channel, it calls all messages in /channels/{channel_id}
    and again in messages/{channel_id}/messages
 - probably more examples
 

https://docs.pinecone.io/guides/data/understanding-metadata#metadata-query-language
$in	Matches vectors with metadata values that are in a specified array. Example: {"genre": {"$in": ["comedy", "documentary"]}}	String, number
use above with channel ids (probably the intersection of channels the sender and receiver are in)